#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ê³¤ì§€ì•” ë¦¬ì¡°íŠ¸ ì‹ìŒì—…ì¥ ìˆ˜ìš”ì˜ˆì¸¡ ê³ ê¸‰ ML ëª¨ë¸ v2.1
- Train ê¸°ë°˜ í†µê³„ë§Œ Testì— ì ìš© (ë°ì´í„° ëˆ„ì„¤ ë°©ì§€)
- ì¹´í…Œê³ ë¦¬ ì•ˆì • ì¸ì½”ë”©(OrdinalEncoder, unknown ì²˜ë¦¬)
- ë¯¸ë˜íŠ¹ì„± ìƒì„± ë¡œì§ê³¼ í•™ìŠµ íŠ¹ì„± ë¡œì§ ì™„ì „ ì¼ì¹˜
- sMAPE ê¸°ë°˜ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° ì•ˆì •í™”
- feature_cols ìŠ¤ëƒ…ìƒ· ê³ ì •ìœ¼ë¡œ ì»¬ëŸ¼ ì •í•©ì„± ë³´ì¥
"""

# ========================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ========================================
import os
import re
import glob
import time
import pickle
import warnings
from dataclasses import dataclass
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')

# ì‹œê°í™” (í•„ìš” ì‹œ ì‚¬ìš©)
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ë¨¸ì‹ ëŸ¬ë‹ ë„êµ¬
from sklearn.preprocessing import RobustScaler, OrdinalEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.utils.validation import check_is_fitted

# ë¶€ìŠ¤íŒ…
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor

# ê³µíœ´ì¼
try:
    import holidays
    KR_HOLIDAYS = holidays.KR(years=range(2023, 2026))
except Exception:
    KR_HOLIDAYS = set()  # í´ë°±: ë¹„ì–´ìˆëŠ” ê³µíœ´ì¼ ì§‘í•©


# ========================================
# 2. í‰ê°€ ë©”íŠ¸ë¦­
# ========================================
def smape(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_true - y_pred) / np.where(denom == 0, 1, denom)
    diff[denom == 0] = 0.0
    return 100 * np.mean(diff)


# ========================================
# 3. ì„¤ì •
# ========================================
@dataclass
class Config:
    train_path: str = './train/train.csv'
    test_dir: str = './test/'
    sample_path: str = './sample_submission.csv'
    out_csv: str = 'advanced_submission_v2_1.csv'

    # ë¶„í• /í•™ìŠµ
    val_hold_days: int = 60      # ë§ˆì§€ë§‰ Nì¼ì„ ê²€ì¦ìœ¼ë¡œ í™€ë“œ
    min_val_rows: int = 100      # ê²€ì¦ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    global_val_ratio: float = 0.85

    # ìƒìœ„ ë©”ë‰´ ê°œë³„ ëª¨ë¸
    top_menu_min_count: int = 100
    top_menu_n: int = 20
    per_menu_min_rows: int = 200
    per_menu_min_val_rows: int = 10

    # ëª¨ë¸ ê³µí†µ
    random_state: int = 42
    n_jobs: int = -1

    # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ
    n_estimators_big: int = 2000

CFG = Config()


# ========================================
# 4. ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ========================================
class DataProcessor:
    def __init__(self):
        self.kr_holidays = KR_HOLIDAYS
        self.menu_stats: Dict[str, Dict[str, float]] = {}
        self.store_stats: Optional[pd.DataFrame] = None
        self.ord_enc: Optional[OrdinalEncoder] = None

    def load_data(self, train_path=CFG.train_path, test_dir=CFG.test_dir):
        print("=" * 50)
        print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")

        train = pd.read_csv(train_path)
        if train['ì˜ì—…ì¼ì'].dtype == 'object':
            train['ì˜ì—…ì¼ì'] = pd.to_datetime(train['ì˜ì—…ì¼ì'])
        print(f"âœ“ Train: {train.shape}")

        test_files = sorted(glob.glob(os.path.join(test_dir, 'TEST_*.csv')))
        test_data = {}
        for fp in test_files:
            name = os.path.basename(fp).replace('.csv', '')
            df = pd.read_csv(fp)
            if df['ì˜ì—…ì¼ì'].dtype == 'object':
                df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
            test_data[name] = df
            print(f"âœ“ {name}: {df.shape}")

        sample = pd.read_csv(CFG.sample_path)
        print(f"âœ“ Sample: {sample.shape}")
        self.sample = sample

        self.train = train
        self.test_data = test_data
        return train, test_data

    # ---------- ê³µí†µ íŠ¹ì„± ----------
    def extract_datetime_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        # date íŒŒìƒ
        if 'date' not in df.columns:
            df['date'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
        else:
            df['date'] = pd.to_datetime(df['date'])

        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['day'] = df['date'].dt.day
        df['dayofweek'] = df['date'].dt.dayofweek
        df['quarter'] = df['date'].dt.quarter
        df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)
        df['dayofyear'] = df['date'].dt.dayofyear

        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
        df['is_holiday'] = df['date'].apply(lambda x: x in self.kr_holidays).astype(int)

        df['day_before_holiday'] = df['date'].apply(lambda x: (x + timedelta(days=1)) in self.kr_holidays).astype(int)
        df['day_after_holiday'] = df['date'].apply(lambda x: (x - timedelta(days=1)) in self.kr_holidays).astype(int)
        df['is_long_weekend'] = ((df['is_holiday'] == 1) |
                                 (df['day_before_holiday'] == 1) |
                                 (df['day_after_holiday'] == 1)).astype(int)

        # ê³„ì ˆ/ì›”ì´ˆì¤‘ë§/ì£¼ì°¨
        df['season'] = df['month'].apply(lambda m: 1 if m in [3,4,5] else 2 if m in [6,7,8] else 3 if m in [9,10,11] else 4)
        df['month_period'] = df['day'].apply(lambda d: 1 if d<=10 else 2 if d<=20 else 3)
        df['week_of_month'] = ((df['day'] - 1) // 7 + 1).astype(int)

        # ì£¼ê¸°ì„±
        df['month_sin'] = np.sin(2*np.pi*df['month']/12)
        df['month_cos'] = np.cos(2*np.pi*df['month']/12)
        df['day_sin'] = np.sin(2*np.pi*df['day']/31)
        df['day_cos'] = np.cos(2*np.pi*df['day']/31)
        df['dayofweek_sin'] = np.sin(2*np.pi*df['dayofweek']/7)
        df['dayofweek_cos'] = np.cos(2*np.pi*df['dayofweek']/7)
        df['weekofyear_sin'] = np.sin(2*np.pi*df['weekofyear']/52)
        df['weekofyear_cos'] = np.cos(2*np.pi*df['weekofyear']/52)

        return df

    def extract_menu_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df['ì—…ì¥ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].apply(lambda x: str(x).split('_')[0])
        df['ë©”ë‰´ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].apply(lambda x: '_'.join(str(x).split('_')[1:]))

        def contains_any(x, words): return int(any(w in str(x) for w in words))
        df['is_ë‹¨ì²´'] = df['ë©”ë‰´ëª…'].str.contains('ë‹¨ì²´', na=False).astype(int)
        df['is_ì •ì‹'] = df['ë©”ë‰´ëª…'].str.contains('ì •ì‹', na=False).astype(int)
        df['is_í›„ì‹'] = df['ë©”ë‰´ëª…'].str.contains('í›„ì‹', na=False).astype(int)
        df['is_ë¸ŒëŸ°ì¹˜'] = df['ë©”ë‰´ëª…'].str.contains('ë¸ŒëŸ°ì¹˜', na=False).astype(int)
        df['is_ì£¼ë¥˜'] = df['ë©”ë‰´ëª…'].apply(lambda x: contains_any(x, ['ë§¥ì£¼','ì†Œì£¼','ë§‰ê±¸ë¦¬','ì™€ì¸','ì£¼ë¥˜','ì¹´ìŠ¤','í…Œë¼','ì°¸ì´ìŠ¬','ì²˜ìŒì²˜ëŸ¼']))
        df['is_ìŒë£Œ'] = df['ë©”ë‰´ëª…'].apply(lambda x: contains_any(x, ['ì½œë¼','ì‚¬ì´ë‹¤','ìŠ¤í”„ë¼ì´íŠ¸','ì»¤í”¼','ì•„ë©”ë¦¬ì¹´ë…¸','ë¼ë–¼','ì—ì´ë“œ','ì°¨']))
        df['is_ë©´ë¥˜'] = df['ë©”ë‰´ëª…'].apply(lambda x: contains_any(x, ['ë©´','ìš°ë™','íŒŒìŠ¤íƒ€','ìŠ¤íŒŒê²Œí‹°','ì§œì¥','ì§¬ë½•']))
        df['is_ê³ ê¸°'] = df['ë©”ë‰´ëª…'].apply(lambda x: contains_any(x, ['ê³ ê¸°','ì‚¼ê²¹','ê°ˆë¹„','ìŠ¤í…Œì´í¬','ë¶ˆê³ ê¸°','ëª©ì‚´']))
        df['ë©”ë‰´ëª…_ê¸¸ì´'] = df['ë©”ë‰´ëª…'].astype(str).str.len()
        return df

    # ---------- Train ê¸°ë°˜ í†µê³„ ----------
    def calculate_train_statistics(self, train_df: pd.DataFrame):
        print("ğŸ“Š Train ê¸°ë°˜ ë©”ë‰´/ì—…ì¥ í†µê³„ ê³„ì‚° ì¤‘...")
        self.menu_stats = {}
        for menu, grp in tqdm(train_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'), desc="ë©”ë‰´ í†µê³„"):
            q = grp['ë§¤ì¶œìˆ˜ëŸ‰']
            self.menu_stats[menu] = {
                'mean': q.mean(),
                'std': q.std(),
                'median': q.median(),
                'min': q.min(),
                'max': q.max(),
                'q25': q.quantile(0.25),
                'q75': q.quantile(0.75),
                'zero_ratio': (q == 0).mean(),
                'positive_ratio': (q > 0).mean(),
            }

        store_stats = train_df.groupby('ì—…ì¥ëª…')['ë§¤ì¶œìˆ˜ëŸ‰'].agg(['mean','std']).reset_index()
        store_stats.columns = ['ì—…ì¥ëª…','store_mean','store_std']
        self.store_stats = store_stats

    def add_train_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        # ë©”ë‰´ í†µê³„
        for stat in ['mean','std','median','zero_ratio','positive_ratio']:
            df[f'menu_{stat}'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].map(
                lambda x: self.menu_stats.get(x, {}).get(stat, 0.0)
            )
        # ì—…ì¥ í†µê³„ (Trainì—ì„œ ê³„ì‚°ëœ í”„ë ˆì„ì„ ë¨¸ì§€)
        if self.store_stats is not None:
            df = df.merge(self.store_stats, on='ì—…ì¥ëª…', how='left')
        else:
            df['store_mean'] = 0.0
            df['store_std'] = 0.0
        return df

    # ---------- ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”© ----------
    def fit_encoders(self, df: pd.DataFrame):
        # ì•ˆì •ì  unknown ì²˜ë¦¬ìš© OrdinalEncoder
        self.ord_enc = OrdinalEncoder(
            handle_unknown='use_encoded_value',
            unknown_value=-1,
            dtype=np.int64,
        )
        cat_cols = ['ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
        self.ord_enc.fit(df[cat_cols])

    def transform_encoders(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        cat_cols = ['ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
        enc = self.ord_enc.transform(df[cat_cols])
        df[[f'{c}_encoded' for c in cat_cols]] = enc
        return df


# ========================================
# 5. ëª¨ë¸ ì•™ìƒë¸”
# ========================================
class AdvancedDemandForecastModel:
    def __init__(self):
        self.models = {}
        self.scalers: Dict[str, RobustScaler] = {}
        self.processor = DataProcessor()
        self.best_params = {}
        self.feature_cols: List[str] = []

    def _create_model_ensemble(self):
        return {
            'xgboost': xgb.XGBRegressor(
                n_estimators=CFG.n_estimators_big,
                max_depth=12,
                learning_rate=0.005,
                subsample=0.85,
                colsample_bytree=0.85,
                min_child_weight=3,
                gamma=0.1,
                reg_alpha=0.1,
                reg_lambda=1,
                random_state=CFG.random_state,
                n_jobs=CFG.n_jobs,
                objective='reg:squarederror',
                early_stopping_rounds=100
            ),
            'lightgbm': lgb.LGBMRegressor(
                n_estimators=CFG.n_estimators_big,
                max_depth=12,
                learning_rate=0.005,
                num_leaves=50,
                min_child_samples=20,
                subsample=0.85,
                colsample_bytree=0.85,
                reg_alpha=0.1,
                reg_lambda=1,
                random_state=CFG.random_state,
                n_jobs=CFG.n_jobs,
                verbose=-1,
                force_col_wise=True
            ),
            'catboost': CatBoostRegressor(
                iterations=CFG.n_estimators_big,
                depth=10,
                learning_rate=0.005,
                l2_leaf_reg=3,
                border_count=128,
                random_seed=CFG.random_state,
                verbose=False,
                early_stopping_rounds=100,
                task_type='CPU'
            ),
            'random_forest': RandomForestRegressor(
                n_estimators=500,
                max_depth=20,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                bootstrap=True,
                oob_score=False,
                random_state=CFG.random_state,
                n_jobs=CFG.n_jobs
            ),
            'extra_trees': ExtraTreesRegressor(
                n_estimators=500,
                max_depth=20,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                bootstrap=False,
                random_state=CFG.random_state,
                n_jobs=CFG.n_jobs
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=1000,
                max_depth=8,
                learning_rate=0.01,
                subsample=0.85,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=CFG.random_state
            ),
        }

    # ---------- íŒŒìƒ/ì¸ì½”ë”© ì¼ê´„ ----------
    def _prepare_features(self, df: pd.DataFrame, is_train=True) -> pd.DataFrame:
        df = self.processor.extract_datetime_features(df)
        df = self.processor.extract_menu_features(df)

        if is_train:
            self.processor.calculate_train_statistics(df)
            self.processor.fit_encoders(df)

        df = self.processor.add_train_statistics(df)
        df = self.processor.transform_encoders(df)
        return df

    def _fit_models(self, X_train, y_train, X_val, y_val, key_for_scaler: str):
        scaler = RobustScaler()
        X_tr = scaler.fit_transform(X_train)
        X_va = scaler.transform(X_val)
        self.scalers[key_for_scaler] = scaler

        models = self._create_model_ensemble()
        results = {}
        start = time.time()

        for name, model in models.items():
            t0 = time.time()
            print(f"  â†’ {name} í•™ìŠµ ì¤‘...", end=" ")

            try:
                if name in ['xgboost','lightgbm','catboost']:
                    if name == 'xgboost':
                        model.fit(X_tr, y_train, eval_set=[(X_va, y_val)], verbose=False)
                    elif name == 'lightgbm':
                        model.fit(X_tr, y_train,
                                  eval_set=[(X_va, y_val)],
                                  callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
                    else:  # catboost
                        model.fit(X_tr, y_train, eval_set=(X_va, y_val), verbose=False)
                else:
                    model.fit(X_tr, y_train)

                pred = model.predict(X_va)
                pred = np.maximum(pred, 0)
                score = smape(y_val, pred)

                results[name] = {'model': model, 'smape': float(score)}
                print(f"ì™„ë£Œ ({time.time()-t0:.1f}ì´ˆ, sMAPE: {score:.2f})")
            except Exception as e:
                print(f"ì‹¤íŒ¨ ({e})")

        print(f"  ì´ í•™ìŠµ ì‹œê°„: {time.time()-start:.1f}ì´ˆ")
        return results

    @staticmethod
    def _calc_weights_by_smape(results: Dict[str, Dict[str, float]]) -> Dict[str, float]:
        # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ sMAPE â†’ 1/(s+1) ê°€ì¤‘ì¹˜
        smapes = {k: v['smape'] for k, v in results.items() if 'smape' in v and np.isfinite(v['smape'])}
        if not smapes:
            return {k: 1.0/len(results) for k in results.keys()}

        inv = {k: 1.0/(s+1.0) for k, s in smapes.items()}
        ssum = sum(inv.values())
        if ssum == 0:
            return {k: 1.0/len(results) for k in results.keys()}
        return {k: v/ssum for k, v in inv.items()}

    def _predict_ensemble(self, models_dict, X, scaler, weights: Optional[Dict[str, float]]=None):
        Xs = scaler.transform(X)
        if weights is None:
            weights = self._calc_weights_by_smape(models_dict)

        preds = []
        for name, info in models_dict.items():
            model = info.get('model')
            if model is None:
                continue
            try:
                p = model.predict(Xs)
                p = np.maximum(p, 0)
                preds.append(p * weights.get(name, 0.0))
            except Exception:
                continue

        if len(preds) == 0:
            return np.zeros(X.shape[0])
        return np.sum(preds, axis=0)

    # ---------- ê³µê°œ ì¸í„°í˜ì´ìŠ¤ ----------
    def fit_global_and_permenu(self, train_df: pd.DataFrame, feature_cols: List[str]):
        self.feature_cols = feature_cols[:]  # ìŠ¤ëƒ…ìƒ· ê³ ì •

        # ì‹œê³„ì—´ ë¶„í• 
        train_df = train_df.sort_values('date')
        split_date = train_df['date'].max() - pd.Timedelta(days=CFG.val_hold_days)

        tr = train_df[train_df['date'] < split_date]
        va = train_df[train_df['date'] >= split_date]
        if len(va) < CFG.min_val_rows:
            split_idx = int(len(train_df) * CFG.global_val_ratio)
            tr = train_df.iloc[:split_idx]
            va = train_df.iloc[split_idx:]

        X_tr = tr[self.feature_cols].fillna(0)
        y_tr = tr['ë§¤ì¶œìˆ˜ëŸ‰']
        X_va = va[self.feature_cols].fillna(0)
        y_va = va['ë§¤ì¶œìˆ˜ëŸ‰']

        print("\n" + "="*50)
        print("ğŸ¯ ì „ì²´ ë°ì´í„° í†µí•© ëª¨ë¸ í•™ìŠµ")
        print("="*50)
        global_models = self._fit_models(X_tr, y_tr, X_va, y_va, key_for_scaler="__GLOBAL__")
        self._print_model_performance(global_models)

        # ìƒìœ„ ë©”ë‰´ ì„ ë³„
        print("\n" + "="*50)
        print("ğŸ¯ ì£¼ìš” ë©”ë‰´ë³„ ê°œë³„ ëª¨ë¸ í•™ìŠµ")
        print("="*50)
        sale_stat = train_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ë§¤ì¶œìˆ˜ëŸ‰'].agg(['sum','count'])
        top = sale_stat[sale_stat['count'] > CFG.top_menu_min_count].sort_values('sum', ascending=False).head(CFG.top_menu_n).index
        print(f"ìƒìœ„ {len(top)}ê°œ ë©”ë‰´ì— ëŒ€í•´ ê°œë³„ ëª¨ë¸ í•™ìŠµ")

        self.menu_models: Dict[str, Dict] = {}
        self.menu_scalers: Dict[str, RobustScaler] = {}

        for menu in tqdm(top, desc="ë©”ë‰´ë³„ ëª¨ë¸"):
            md = train_df[train_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu].sort_values('date')
            if len(md) < CFG.per_menu_min_rows:
                continue
            trm = md[md['date'] < split_date]
            vam = md[md['date'] >= split_date]
            if len(vam) < CFG.per_menu_min_val_rows:
                continue

            X_trm = trm[self.feature_cols].fillna(0)
            y_trm = trm['ë§¤ì¶œìˆ˜ëŸ‰']
            X_vam = vam[self.feature_cols].fillna(0)
            y_vam = vam['ë§¤ì¶œìˆ˜ëŸ‰']

            res = self._fit_models(X_trm, y_trm, X_vam, y_vam, key_for_scaler=f"MENU::{menu}")
            self.menu_models[menu] = res
            self.menu_scalers[menu] = self.scalers[f"MENU::{menu}"]

        # ìµœì¢… (í™€ë“œì•„ì›ƒ 10%)
        print("\n" + "="*50)
        print("ğŸ¯ ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)")
        print("="*50)
        X_full = train_df[self.feature_cols].fillna(0)
        y_full = train_df['ë§¤ì¶œìˆ˜ëŸ‰']
        val_size = max(int(len(X_full)*0.1), 1000) if len(X_full) > 1000 else max(int(len(X_full)*0.1), 50)

        final = self._fit_models(
            X_full[:-val_size], y_full.iloc[:-val_size],
            X_full[-val_size:],  y_full.iloc[-val_size:],
            key_for_scaler="__FINAL__"
        )
        self.final_models = final
        return global_models, final

    def predict_for_tests(self, test_dict: Dict[str, pd.DataFrame], feature_cols: List[str]) -> List[Dict]:
        preds_out = []
        for test_name, test_df in test_dict.items():
            print(f"\nâ†’ {test_name} ì˜ˆì¸¡ ì¤‘...")
            test_fe = self._prepare_features(test_df, is_train=False)

            last_date = pd.to_datetime(test_df['ì˜ì—…ì¼ì'].max())
            menus = test_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].unique().tolist()

            for day_ahead in range(1, 8):
                pred_date = last_date + timedelta(days=day_ahead)

                for menu in menus:
                    last_row = test_fe[test_fe['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu].tail(1).copy()
                    future_row = self._create_future_row(last_row, pred_date)
                    # ì»¬ëŸ¼ ì •í•©ì„±
                    for c in feature_cols:
                        if c not in future_row.columns:
                            future_row[c] = 0
                    future_X = future_row[feature_cols].fillna(0)

                    # ë©”ë‰´ ì „ìš© ë˜ëŠ” ìµœì¢… ì‚¬ìš©
                    if hasattr(self, 'menu_models') and (menu in self.menu_models):
                        pred = self._predict_ensemble(self.menu_models[menu], future_X, self.menu_scalers[menu])
                    else:
                        pred = self._predict_ensemble(self.final_models, future_X, self.scalers["__FINAL__"])

                    pred_value = float(pred[0]) if np.ndim(pred) > 0 else float(pred)

                    # í›„ì²˜ë¦¬: Train í†µê³„ ê¸°ë°˜
                    ms = self.processor.menu_stats.get(menu, {})
                    mmax = ms.get('max', np.inf)
                    pred_value = np.clip(pred_value, 0, (mmax if np.isfinite(mmax) else pred_value) * 1.5)

                    if pred_date.dayofweek >= 5:
                        pred_value *= 1.2
                    if pred_date in self.processor.kr_holidays:
                        pred_value *= 1.3

                    preds_out.append({
                        'ì˜ì—…ì¼ì': f"{test_name}+{day_ahead}ì¼",
                        'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': menu,
                        'ë§¤ì¶œìˆ˜ëŸ‰': int(round(max(0.0, pred_value)))
                    })
        return preds_out

    def _create_future_row(self, last_row: pd.DataFrame, future_date: pd.Timestamp) -> pd.DataFrame:
        # last_rowì˜ ì¹´í…Œê³ ë¦¬/ë©”ë‰´ íŠ¹ì„±ì€ ìœ ì§€, ë‚ ì§œê³„ì—´ íŠ¹ì„±ë§Œ ì¬ê³„ì‚°
        row = last_row.copy()
        row['ì˜ì—…ì¼ì'] = future_date
        row['date'] = future_date

        # ë‚ ì§œ íŠ¹ì„± ì¬ê³„ì‚°: ë™ì¼ í•¨ìˆ˜ ì‚¬ìš©
        row = self.processor.extract_datetime_features(row)

        # ì£¼ê¸°ì /êµ¬ê°„ íŠ¹ì„± ë“±ì€ extract_datetime_featuresê°€ ëª¨ë‘ ê³„ì‚°
        # ë©”ë‰´ ê´€ë ¨ í†µê³„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (Train ê¸°ë°˜)
        # ì¸ì½”ë”©ëœ ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì€ last_rowì—ì„œ ìœ ì§€ë¨

        return row

    @staticmethod
    def _print_model_performance(results: Dict[str, Dict[str, float]]):
        print("\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (sMAPE)")
        print("-"*40)
        sorted_items = sorted(
            [(k, v) for k, v in results.items() if 'smape' in v],
            key=lambda x: x[1]['smape']
        )
        for name, info in sorted_items:
            print(f"  {name:20s}: {info['smape']:.4f}")
        if sorted_items:
            best = sorted_items[0]
            print("-"*40)
            print(f"  ğŸ¥‡ ìµœê³  ì„±ëŠ¥: {best[0]} ({best[1]['smape']:.4f})")


# ========================================
# 6. íŒŒì´í”„ë¼ì¸
# ========================================
class Pipeline:
    def __init__(self):
        self.model = AdvancedDemandForecastModel()

    def run(self):
        print("=" * 70)
        print("ğŸš€ ê³¤ì§€ì•” ë¦¬ì¡°íŠ¸ ì‹ìŒì—…ì¥ ìˆ˜ìš”ì˜ˆì¸¡ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (v2.1)")
        print("=" * 70)
        t0 = time.time()

        # 1) ë¡œë“œ
        train, test_dict = self.model.processor.load_data()

        # 2) íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Train)
        print("\nğŸ“ˆ Train íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")
        train_fe = self.model._prepare_features(train, is_train=True)

        # 3) ë¶„ì„(ê°„ë‹¨ ë¡œê·¸)
        self._simple_eda(train_fe)

        # 4) í•™ìŠµì— ì‚¬ìš©í•  íŠ¹ì„± ê³ ì •
        exclude = ['ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ë§¤ì¶œìˆ˜ëŸ‰','date','ì—…ì¥ëª…','ë©”ë‰´ëª…']
        feature_cols = [c for c in train_fe.columns if c not in exclude]
        self.model.feature_cols = feature_cols[:]
        print(f"\nğŸ“Š ì‚¬ìš©í•  íŠ¹ì„± ìˆ˜: {len(feature_cols)}")

        # 5) ê¸€ë¡œë²Œ & ë©”ë‰´ë³„ í•™ìŠµ
        global_models, final_models = self.model.fit_global_and_permenu(train_fe, feature_cols)

        # 6) í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        print("\n" + "="*50)
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡")
        print("="*50)
        preds = self.model.predict_for_tests(test_dict, feature_cols)

        # 7) ì œì¶œ íŒŒì¼ ìƒì„±
        self._create_submission(preds)

        print("\n" + "="*70)
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (ì´ ì†Œìš”: {(time.time()-t0)/60:.1f}ë¶„)")
        print("="*70)
        return final_models

    def _simple_eda(self, df: pd.DataFrame):
        print("\nğŸ“Š ë°ì´í„° ìš”ì•½")
        print("[ë§¤ì¶œìˆ˜ëŸ‰ í†µê³„]")
        print(df['ë§¤ì¶œìˆ˜ëŸ‰'].describe())

        print("\n[ì—…ì¥ë³„ ë§¤ì¶œ TOP 5]")
        st = df.groupby('ì—…ì¥ëª…')['ë§¤ì¶œìˆ˜ëŸ‰'].agg(['sum','mean']).sort_values('sum', ascending=False).head(5)
        print(st)

        print("\n[ìš”ì¼ë³„ í‰ê·  ë§¤ì¶œ]")
        wk = df.groupby('dayofweek')['ë§¤ì¶œìˆ˜ëŸ‰'].mean()
        weekdays = ['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼']
        for d, v in wk.items():
            print(f"  {weekdays[d]}: {v:.2f}")

    def _create_submission(self, pred_list: List[Dict]):
        print("\nğŸ“‹ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
        pred_df = pd.DataFrame(pred_list)
        sample = self.model.processor.sample.copy()
        sample.iloc[:, 1:] = 0

        # ì±„ìš°ê¸°
        # ì„±ëŠ¥: í–‰ ì¸ë±ìŠ¤ ë§µ/ì—´ ë§µì„ ë§Œë“¤ì–´ ë²¡í„°í™”í•  ìˆ˜ë„ ìˆìœ¼ë‚˜, ì•ˆì „í•˜ê²Œ ë£¨í”„ ìœ ì§€
        idx_map = {d: i for i, d in enumerate(sample['ì˜ì—…ì¼ì'].values)}
        col_set = set(sample.columns[1:])

        for _, r in pred_df.iterrows():
            d = r['ì˜ì—…ì¼ì']
            m = r['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
            v = r['ë§¤ì¶œìˆ˜ëŸ‰']
            if (d in idx_map) and (m in col_set):
                sample.at[idx_map[d], m] = v

        sample.to_csv(CFG.out_csv, index=False, encoding='utf-8-sig')
        print(f"âœ“ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {CFG.out_csv}")


# ========================================
# 7. ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    pipeline = Pipeline()
    final_models = pipeline.run()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"  - {CFG.out_csv}: ì œì¶œìš© ì˜ˆì¸¡ íŒŒì¼")